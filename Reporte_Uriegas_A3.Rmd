---
title: "Actividad 3"
output: #html_document
#  md_document:
#    variant: markdown_github

date: '`r format(Sys.time(), "%d %B, %Y")`'
author:
  - Alumno.- Jesús Eduardo Uriegas Ibarra
---

# Regresión Lineal Multiple
*Coeficiente de determinación:* explica que tan bueno es el modelo para predecir; técnicamente es el porcentaje de varianza de $Y$ con respecto a la varianza del modelo.

Existen diferentes formas de seleccionar parámetros, el que se incluye en R y esta basado en un modelo matemático es el método paso a paso o stepwise.

## Ejemplo 1
### Problema
Un estudio quiere generar un modelo que permita predecir la esperanza de vida media de los habitantes de una ciudad en función de diferentes variables. Se dispone de información sobre: habitantes, analfabetismo, ingresos, esperanza de vida, asesinatos, universitarios, heladas, área y densidad poblacional.

Primero se cargan los datos y se renombran para las columnas dado que tiene nombres en ingles:
```{r}
library(dplyr)
datos <- as.data.frame(state.x77)
datos <- rename(habitantes = Population, analfabetismo = Illiteracy,
                ingresos = Income, esp_vida = `Life Exp`, asesinatos = Murder,
                universitarios = `HS Grad`, heladas = Frost, area = Area,
                .data = datos)
datos <- mutate(.data = datos, densidad_pobl = habitantes * 1000 / area)
```

Analisis de la relación entre las variables.
```{r}
round(cor(x = datos, method = "pearson"), 3)
```
El anterior código muestra una tabla de correlación entre cada variable, aún con una correlación positiva o negativa no se puede decir que las variables estan correlacionadas porque primero se debe evaluar la significancia estadística de esta relacion (p-value).

Para visualizar de mejor manera lo anterior se puede graficar las sigueintes 2 figuras:
```{r}
library(psych)
multi.hist(x = datos, dcol = c("blue", "red"), dlty = c("dotted", "solid"),
           main = "")
```
```{r}
library(GGally)
ggpairs(datos, lower = list(continuous = "smooth"),
        diag = list(continuous = "barDiag"), axisLabels = "none")
```
A simple vista se observa que para la esperanza de vida (Y) las variables que más correlación tienen con esta (las que más son causa de) son los asesinatos, analfabetismo y universitarios.

Ahora bien, se crea el modelo de regresión lineal considerando todas las variables para predecir la experanza de vida:
```{r}
modelo <- lm(esp_vida ~ habitantes + ingresos + analfabetismo + asesinatos +
               universitarios + heladas + area + densidad_pobl, data = datos )
summary(modelo)
```
Se imprimen los datos y los valores t-value de cada variable, donde algunas no parecen tener mucha correlación, aún así la significancia estadística del modelo (p-value) es relevante 0.003787.

El modelo que se acaba de generar cuenta con variables que en realidad no ayudan mucho a explicar la esperanza de vida, lo que busca es deshacernos de ella, pero para ello hay que establecer una metodología para saber que parámetros no son relevantes, aquí se emplea el método paso a paso:
```{r}
step(object = modelo, direction = "both", trace = 1)
```
Lo que hizó el código es evaluar el modelo dando pasos hasta llegar al modelo siguiente que es el que se considera más ideal para expresar la esperanza de vida:

```{r}
modelo <- (lm(formula = esp_vida ~ habitantes + asesinatos + universitarios +
              heladas, data = datos))
summary(modelo)
```
Al comparar este método con el visual que se realizó arriba con las gráficas se observa que las 3 variables que se mencionaron eran representativas aquí se incluyen, con la diferencia de que aquí se incluye una variable más.

El intervalo de confianza muestra donde se encuentran los datos probablemente:
```{r}
confint(lm(formula = esp_vida ~ habitantes + asesinatos + universitarios +
            heladas, data = datos))
```
Aquí se observa un intervalo bastante cerrado, lo cual es bueno.

Ahora bien, es necesario evaluar que los predictores (variables independientes) de verdad estan tienen una relación lineal con la variable respuesta (esperanza de vida), para ello se grafican los residuos de cada variable independiente:

```{r}
library(ggplot2)
library(gridExtra)
plot1 <- ggplot(data = datos, aes(habitantes, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()
plot2 <- ggplot(data = datos, aes(asesinatos, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()
plot3 <- ggplot(data = datos, aes(universitarios, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()
plot4 <- ggplot(data = datos, aes(heladas, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()
grid.arrange(plot1, plot2, plot3, plot4)
```
Como se observam todos los predictores cuentan con lineadlidad a lo largo de 0.

Otros test para evaluar la linealidad del modelo son el método gráfico de QQline y el test de Shapiro:
```{r}
qqnorm(modelo$residuals)
qqline(modelo$residuals)
```
La imagen muestra linealidad entre los cuantiles teóricos y testeo.
```{r}
shapiro.test(modelo$residuals)
```
Se observa que l p-value es 0.525, por lo que el modelo es valido para linealidad.

Para evaluar la homocedasticidad se puede emplear la siguiente gráfica de residuos:
```{r}
ggplot(data = datos, aes(modelo$fitted.values, modelo$residuals)) +
geom_point() +
geom_smooth(color = "firebrick", se = FALSE) +
geom_hline(yintercept = 0) +
theme_bw()
```
Igualmente se puede emplear el método de test:
```{r}
library(lmtest)
bptest(modelo)
```
Lo cual indica que existe homocedasticidad.

Por otra parte, para evaluar la correlación entre los predictores (si hay correlación es malo) se puede emplear la siguiente gráfica:
```{r}
library(corrplot)
corrplot(cor(dplyr::select(datos, habitantes, asesinatos,universitarios,heladas)),
         method = "number", tl.col = "black")
```
Se observa que la correlación es negativa en la mayoria de los casos, aunque lijeramente positiva en otros casos, aunque no es una correlación elevada.


```{r}
library(car)
vif(modelo)
```
En el test de inflación de varianza tampoco se observan datos relevantes de inflación de varianza.

Evaluando si existe autocorrelación:
```{r}
library(car)
dwt(modelo, alternative = "two.sided")
```
Se observa que no hay evidenia de ello ya que el p-value es de 0.8

Observando si es que existen valores atípicos:
```{r}
library(dplyr)
datos$studentized_residual <- rstudent(modelo)
ggplot(data = datos, aes(x = predict(modelo), y = abs(studentized_residual))) +
geom_hline(yintercept = 3, color = "grey", linetype = "dashed") +
geom_point(aes(color = ifelse(abs(studentized_residual) > 3, 'red', 'black'))) +
scale_color_identity() +
labs(title = "Distribución de los residuos studentized",
     x = "predicción modelo") + 
theme_bw() + theme(plot.title = element_text(hjust = 0.5))
```
Se observa que ningún dato es mayor a 3, lo que indica que no hay observaciones atípicas, esto se puede comprobar corriendo el siguiente código:
```{r}
which(abs(datos$studentized_residual) > 3)
```
Para evaluar si existe influencia entre las variables se puede emplear el siguiente resumen de un modelo de influencia:
```{r}
summary(influence.measures(modelo))
```
Lo que muestra es la influencia según diferentes metologías, para visualizar esto:
```{r}
influencePlot(modelo)
```
Aquí se observa que hay valors relevantes como el de California y Hawaii.

## Ejemplo 2
### Problema
Se dispone de un dataset que contiene información de 30 libros. Se conoce del peso total de cada libro, el volumen que tiene y el tipo de tapas (duras o blandas). Se quiere generar un modelo lineal múltiple que permita predecir el peso de un libro en función de su volumen y del tipo de tapas.

Nuevamente, primero se deben de crear o extraer los datos, en este caso se crean:
```{r}
datos <- data.frame(peso = c(800, 950, 1050, 350, 750, 600, 1075, 250, 700,
                             650, 975, 350, 950, 425, 725),
                    volumen = c(885, 1016, 1125, 239, 701, 641, 1228, 412, 953,
                                929, 1492, 419, 1010, 595, 1034),
                    tipo_tapas = c("duras", "duras", "duras", "duras", "duras", 
                                   "duras", "duras", "blandas", "blandas",
                                   "blandas", "blandas", "blandas", "blandas",
                                   "blandas", "blandas"))
head(datos, 4)
```
Se aplica la misma metodologia que en el método pasado.
Primero se analiza la correlación entre las variables.
```{r}
datos$tipo_tapas <- as.factor(datos$tipo_tapas)
pairs(x = datos)
```
Visualmente en la gráfica se observa que hay crrelación entre peso y el volúmen.
Por otra parte, haciendo una prueba de correlación de tipo pearson:
```{r}
cor.test(datos$peso, datos$volumen, method = "pearson")
```
Se tiene que también hay signficancia entre la correlación de la variable dependiente y estas 2 variables.
Por último, el modelo de velas muestra lo siguiente:
```{r}
ggplot(data = datos, mapping=aes(x = tipo_tapas, y = peso, color=tipo_tapas)) +
geom_boxplot() +
geom_jitter(width = 0.1) +
theme_bw() + theme(legend.position = "none")
```
Lo que muestra es que la variable tipo de tapa influye en la variable peso, lo que hay que tener en cuenta a la hora de crear el modelo ya que existiria correlación entre las variables independientes.

Creando el modelo se tiene:
```{r}
modelo <- lm(peso ~ volumen + tipo_tapas, data = datos)
summary(modelo)
```
En este caso el p-value es mayor al del ejemplo mayor, pero aún así tiene relevancia.

Ahora, elijiendo los predictores (variables independientes) se pueden aplicar los mismos modelos del ejemplo 1 quedando:
```{r}
library(ggplot2)
ggplot(data = datos, aes(x = volumen, y = modelo$residuals)) +
geom_point() +
geom_smooth(color = "firebrick") +
geom_hline(yintercept = 0) +
theme_bw()
```
Lo que muestra que hay linealidad en el modelo, aunque disminuye al final al aumentar el volumen.

Aplicando la gráfica Normal QQ:
```{r}
qqnorm(modelo$residuals)
qqline(modelo$residuals)
```
También se observa linealidad.

Nuevamente, el test Shapiro, el cual ahora también muestra linealidad aunque no satisface la normalidad.
```{r}
shapiro.test(modelo$residuals)
```
Por lo anterior es mejor excluir el valor que se considera atípico, y volviendo a aplicar el test:
```{r}
which.max(modelo$residuals)
shapiro.test(modelo$residuals[-13])
```
Ahora si se observa normalidad al ser el valor de p más elevado.

Evaluando la variabilidad de de las constantes de los residuos se observa:
```{r}
ggplot(data = data.frame(predict_values = predict(modelo),
                         residuos = residuals(modelo)),
       aes(x = predict_values, y = residuos)) +
    geom_point() +
    geom_smooth(color = "firebrick", se = FALSE) +
    geom_hline(yintercept = 0) +
    theme_bw()
```

Igualmente al hacer la prueba se observa que existe homocedasticidad:
```{r}
library(lmtest)
bptest(modelo)
```

Con la misma prueba de correlación se observa de igual manera que no existe correlación entre las variables:
```{r}
library(car)
dwt(modelo,alternative = "two.sided")
```

Para identificar los valores atípicos se emplea el siguiente test:
```{r}
library(car)
outlierTest(modelo)
```
Se observa que el test devuelve la fila 13, la cual se tuvo que eliminar anteriormente para seguir con los test de normalidad, ahora con este outlier se debe saber si es influyente aplicando el siguiente código:
```{r}
summary(influence.measures(modelo))
```
Ahora graficando lo anterior.

```{r}
influencePlot(modelo)
```
El test revela que el dato 13 es un outlier relevente dado que se encuentra alejado de los demás en el mapa, aunque no se excede los límites para considerarlo influyente.
